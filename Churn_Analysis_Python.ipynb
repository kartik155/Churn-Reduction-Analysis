{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ragnarok\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Importin Libraries\n",
    "\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting working Directory\n",
    "os.chdir(\"D:\\Data Science\\edWisor\\Churn-Reduction-Analysis\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Data\n",
    "\n",
    "train = pd.read_csv(\"Train_data.csv\")\n",
    "test = pd.read_csv(\"Test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.columns = train.columns.str.replace(' ','_')\n",
    "test.columns = test.columns.str.replace(' ','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_train = train.copy()\n",
    "original_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['state', 'account_length', 'area_code', 'phone_number',\n",
       "       'international_plan', 'voice_mail_plan', 'number_vmail_messages',\n",
       "       'total_day_minutes', 'total_day_calls', 'total_day_charge',\n",
       "       'total_eve_minutes', 'total_eve_calls', 'total_eve_charge',\n",
       "       'total_night_minutes', 'total_night_calls', 'total_night_charge',\n",
       "       'total_intl_minutes', 'total_intl_calls', 'total_intl_charge',\n",
       "       'number_customer_service_calls', 'Churn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features.to_csv('features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperating Continous and categorical variables for analysis\n",
    "\n",
    "cnames = ['account_length', 'number_vmail_messages', 'total_day_minutes', 'total_day_calls',\n",
    "          'total_day_charge', 'total_eve_minutes', 'total_eve_calls', 'total_eve_charge',\n",
    "          'total_night_minutes', 'total_night_calls', 'total_night_charge', 'total_intl_minutes',\n",
    "          'total_intl_calls', 'total_intl_charge', 'number_customer_service_calls']\n",
    "cat_names = ['state','area_code','international_plan', 'voice_mail_plan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Density plots of continous variables\n",
    "\n",
    "fig,axes = plt.subplots(nrows = 5, ncols = 3, figsize = (32,36)) \n",
    "k=0\n",
    "for i in range(5):\n",
    "    for j in range(3):\n",
    "        axes[i,j].hist(train[cnames[i+j+k]], bins =30)\n",
    "        axes[i,j].set_title(cnames[i+j+k].replace('_',' '), fontsize = 25)\n",
    "        axes[i,j].set_ylabel('Count', fontsize = 20)\n",
    "    k=k+2\n",
    "plt.tight_layout\n",
    "#plt.savefig('Distributionplots.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting 'number_vmail_messages' without zero value\n",
    "\n",
    "df = train.loc[train['number_vmail_messages']>0,'number_vmail_messages']\n",
    "plt.hist(df, bins = 20)\n",
    "plt.ylabel('Count', fontsize = 20)\n",
    "plt.xlabel('Messages', fontsize = 20)\n",
    "#plt.savefig('voicemail.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Log transforming the skewed variables (if needed)\n",
    "\n",
    "#for  i in ['number_vmail_messages', 'total_intl_calls', 'number_customer_service_calls']:\n",
    "#    X = train[i].values + 1 \n",
    "#    train[i] = np.log(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Z-Score transform (if needed)\n",
    "\n",
    "#from scipy import stats\n",
    "#train['number_vmail_messages'] = stats.zscore(train['number_vmail_messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking corelations of continous variables\n",
    "\n",
    "c_corr = train[cnames].corr()\n",
    "plt.figure(figsize = (60,60))\n",
    "sns.set(font_scale = 3.8)\n",
    "\n",
    "sns.heatmap(c_corr, cmap='magma', linecolor='white', linewidth=5, square = True,\n",
    "            xticklabels = list(pd.Index(cnames).str.replace('_',' ')),\n",
    "            yticklabels = list(pd.Index(cnames).str.replace('_',' ')))\n",
    "#plt.savefig('Corelations.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking dependency of dependent variable on categorical variables\n",
    "\n",
    "for i in cat_names:\n",
    "    print(i)\n",
    "    chi2, p, dof, ex = chi2_contingency(pd.crosstab(train['Churn'], train[i]))\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking counts of Target variable\n",
    "plt.figure(figsize = (9,6))\n",
    "sns.set(font_scale = 1)\n",
    "sns.countplot(x = 'Churn', data = train)\n",
    "plt.xlabel('Churn', fontsize = 20)\n",
    "plt.ylabel('Counts', fontsize = 20)\n",
    "#plt.savefig('TargetCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot of Number of voicemail messages by Class\n",
    "\n",
    "plt.figure(figsize = (10,15))\n",
    "train.hist('number_vmail_messages', by = 'Churn')\n",
    "plt.ylabel('Count', fontsize = 20)\n",
    "#plt.savefig('voicemailClass.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot of Total Intl calls by Class\n",
    "\n",
    "plt.figure(figsize = (10,15))\n",
    "train.hist('total_intl_calls', by = 'Churn')\n",
    "plt.ylabel('Count', fontsize = 20)\n",
    "#plt.savefig('intlcallsClass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot of Number of customer service calls by Class\n",
    "\n",
    "plt.figure(figsize = (10,15))\n",
    "train.hist('number_customer_service_calls', by = 'Churn')\n",
    "plt.ylabel('Count', fontsize = 20)\n",
    "#plt.savefig('servivecallsClass.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot of States\n",
    "\n",
    "plt.figure(figsize = (15,10))\n",
    "sns.countplot('state', data= original_train)\n",
    "plt.xlabel('State', fontsize = 20)\n",
    "plt.ylabel('Count', fontsize = 20)\n",
    "#plt.savefig('state.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features by normalising\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler_norm = MinMaxScaler()\n",
    "#for i in cnames:\n",
    "#    train[i] = scaler_norm.fit_transform(train[i].values.reshape(-1,1))\n",
    "#    test[i] = scaler_norm.fit_transform(test[i].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping the irrelevant variables\n",
    "\n",
    "drop_col = ['total_day_minutes', 'total_eve_minutes', 'total_night_minutes',\n",
    "            'total_intl_minutes', 'area_code', 'phone_number']\n",
    "train.drop(drop_col, axis = 1, inplace = True)\n",
    "test.drop(drop_col, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing 'Yes','No','True','False' with 1 and 0\n",
    "\n",
    "train['international_plan'] = train['international_plan'].replace(' yes', 1).replace(' no', 0)\n",
    "train['voice_mail_plan'] = train['voice_mail_plan'].replace(' yes', 1).replace(' no', 0)\n",
    "train['Churn'] = train['Churn'].replace(' False.', 0).replace(' True.', 1)\n",
    "test['international_plan'] = test['international_plan'].replace(' yes', 1).replace(' no', 0)\n",
    "test['voice_mail_plan'] = test['voice_mail_plan'].replace(' yes', 1).replace(' no', 0)\n",
    "test['Churn'] = test['Churn'].replace(' False.', 0).replace(' True.', 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating dummy variables of state variable (if needed)\n",
    "\n",
    "#temp_1 = pd.get_dummies(train['state'], prefix = 'state')\n",
    "#train = train.join(temp_1)\n",
    "#temp_2 = pd.get_dummies(test['state'], prefix = 'state')\n",
    "#test = test.join(temp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning a code to each state\n",
    "\n",
    "keys = train['state'].unique().tolist()\n",
    "values = list(range(len(keys)))\n",
    "state_codes = dict(zip(keys,values))\n",
    "train['state'] = train['state'].map(state_codes)\n",
    "test['state'] = test['state'].map(state_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(train['state'], bins = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Data for model training\n",
    "\n",
    "train_var = train.columns\n",
    "train_data_X = train[train_var].drop('Churn', axis = 1)\n",
    "train_data_Y = train['Churn']\n",
    "test_data_X = test[train_var].drop('Churn', axis = 1)\n",
    "test_data_Y = test['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Over sampling the complete data to deal with target class imbalance\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(train_data_Y==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(train_data_Y==0)))\n",
    "\n",
    "train_data_X_over, train_data_Y_over = smote.fit_sample(train_data_X, train_data_Y.ravel())\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(train_data_Y_over==1)))\n",
    "print(\"After OverSampling, counts of label '0': {} \\n\".format(sum(train_data_Y_over==0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Custom Function for Accuracy and FNR\n",
    "\n",
    "def conf_matrix(y,pred):\n",
    "    CM = pd.crosstab(y,pred)\n",
    "    \n",
    "    Accuracy = (sum(np.diag(CM)) * 100)/len(pred)\n",
    "    FNR = (CM.iloc[1,0] *100)/sum(CM.iloc[1,])\n",
    "    \n",
    "    #print(CM)\n",
    "    #print('Accuracy : {:.3f}'.format(Accuracy))\n",
    "    #print('FNR : {:.3f}'.format(FNR))\n",
    "    return (Accuracy,FNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for auc\n",
    "\n",
    "def auc_val(y,pred):\n",
    "    fpr,tpr,thresholds = roc_curve(y,pred)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    auc_4f = round(roc_auc,4)\n",
    "    return (auc_4f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the training data for model evaluation\n",
    "\n",
    "X_train_under, X_valid, y_train_under, y_valid = train_test_split(train_data_X, train_data_Y,\n",
    "                                                    stratify = train_data_Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Over sampling for models to deal with target class imbalance\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train_under==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train_under==0)))\n",
    "\n",
    "X_train, y_train = smote.fit_sample(X_train_under, y_train_under.ravel())\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "print(\"After OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decission Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search to find best max_depth\n",
    "\n",
    "best_fnr = 100\n",
    "dt_train_auc = []\n",
    "dt_test_auc = []\n",
    "\n",
    "max_dep = [6, 8, 10, 12, 15, 18, 20]\n",
    "\n",
    "for max_depth in max_dep:\n",
    "    clf = DecisionTreeClassifier(criterion = 'entropy', max_depth = max_depth,\n",
    "                                 random_state = 0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    temp1_pred = clf.predict(X_train)\n",
    "    dt_train_auc.append(auc_val(y_train,temp1_pred))\n",
    "    tr_acc = clf.score(X_train,y_train)\n",
    "    print('Training Accuracy : {:.3f}'.format(tr_acc))\n",
    "    temp2_pred = clf.predict(X_valid)\n",
    "    dt_test_auc.append(auc_val(y_valid, temp2_pred))\n",
    "    Acc,fnr = conf_matrix(y_valid, temp2_pred)\n",
    "    print('---')\n",
    "    if (((Acc > 80) & (tr_acc < 1)) & (fnr < best_fnr)):\n",
    "        best_fnr = fnr\n",
    "        best_params = {'max_depth': max_depth}\n",
    "\n",
    "print('Best FNR : {:.2f}'.format(best_fnr))\n",
    "print('Best_FNR_parameters : {}'.format(best_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC grids\n",
    "\n",
    "dt_train_auc = pd.DataFrame(dt_train_auc, index = max_dep)\n",
    "dt_test_auc = pd.DataFrame(dt_test_auc, index = max_dep)\n",
    "print('Training AUC')\n",
    "print(dt_train_auc)\n",
    "print('Test AUC')\n",
    "print(dt_test_auc)\n",
    "dt_test_auc.to_csv('DT_AUC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model with best parameters based on test_auc on complete training data\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion = 'entropy', max_depth = 10, random_state = 0)\n",
    "\n",
    "tree.fit(train_data_X_over, train_data_Y_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Accuracy\n",
    "\n",
    "tree.score(train_data_X_over, train_data_Y_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on Actual Test data\n",
    "\n",
    "pred_dt = tree.predict(test_data_X)\n",
    "\n",
    "Accuracy_dt,FNR_dt = conf_matrix(test_data_Y,pred_dt)\n",
    "\n",
    "dt_auc = auc_val(test_data_Y,pred_dt)\n",
    "\n",
    "print('Test Accuracy: {:.3f}'.format(Accuracy_dt))\n",
    "print('Test FNR: {:.3f}'.format(FNR_dt))\n",
    "print('Test AUC: {:.3f}'.format(dt_auc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for finding best parameters\n",
    "\n",
    "best_fnr = 100\n",
    "\n",
    "n_estimators = [40, 60, 80, 100, 200]\n",
    "m_depth = [6, 8, 10, 12, 15, 18, 20]\n",
    "\n",
    "rf_train_auc = np.zeros((len(n_estimators),len(m_depth)))\n",
    "rf_test_auc = np.zeros((len(n_estimators),len(m_depth)))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for n_est in n_estimators:\n",
    "    j = 0\n",
    "    for max_d in m_depth:\n",
    "        clf = RandomForestClassifier(n_estimators = n_est, max_features = 'sqrt',\n",
    "                                     oob_score = True, max_depth = max_d, criterion = 'entropy',\n",
    "                                     random_state = 0)\n",
    "        clf.fit(X_train, y_train)\n",
    "        temp1_pred = clf.predict(X_train)\n",
    "        rf_train_auc[i,j] = auc_val(y_train, temp1_pred)\n",
    "        tr_acc = clf.score(X_train,y_train)\n",
    "        print('Training Accuracy : {:.3f}'.format(tr_acc))\n",
    "        temp2_pred = clf.predict(X_valid)\n",
    "        rf_test_auc[i,j] = auc_val(y_valid, temp2_pred)\n",
    "        Acc,fnr = conf_matrix(y_valid, temp2_pred)\n",
    "        print('---')\n",
    "        if (((Acc > 80) & (tr_acc < 1)) & (fnr < best_fnr)):\n",
    "            best_fnr = fnr\n",
    "            best_params = {'max_depth' : max_d, 'n_estimators' : n_est}\n",
    "        j = j+1\n",
    "    i = i+1\n",
    "\n",
    "print('Best FNR : {:.2f}'.format(best_fnr))\n",
    "print('Best_FNR_parameters : {}'.format(best_params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC grids\n",
    "\n",
    "rf_train_auc = pd.DataFrame(rf_train_auc, index = n_estimators, columns = m_depth)\n",
    "rf_test_auc = pd.DataFrame(rf_test_auc, index = n_estimators, columns = m_depth)\n",
    "print('Training AUC')\n",
    "print(rf_train_auc)\n",
    "print('Test AUC')\n",
    "print(rf_test_auc)\n",
    "rf_test_auc.to_csv('RF_AUC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining the model for full training data with best parameters based on test_auc\n",
    "\n",
    "rf_tree = RandomForestClassifier(n_estimators = 80, max_features = 'sqrt', oob_score = True, max_depth = 6, criterion = 'entropy', random_state = 0)\n",
    "\n",
    "rf_tree.fit(train_data_X_over, train_data_Y_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Score\n",
    "\n",
    "rf_tree.score(train_data_X_over, train_data_Y_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on test data\n",
    "\n",
    "pred_rf = rf_tree.predict(test_data_X)\n",
    "\n",
    "Accuracy_rf,FNR_rf = conf_matrix(test_data_Y,pred_rf)\n",
    "\n",
    "rf_auc = auc_val(test_data_Y,pred_rf)\n",
    "\n",
    "print('Test Accuracy: {:.3f}'.format(Accuracy_rf))\n",
    "print('Test FNR: {:.3f}'.format(FNR_rf))\n",
    "print('Test AUC: {:.3f}'.format(rf_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "logit = sm.Logit(y_train, X_train).fit()\n",
    "\n",
    "logit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building ROC curve to decide the threshold value for classification\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "fpr,tpr,thresholds = roc_curve(y_valid, logit.predict(X_valid))\n",
    "plt.figure(figsize = (9,6))\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlabel('False Positive Rate', fontsize = 15)\n",
    "plt.ylabel('True Positive Rate', fontsize = 15)\n",
    "plt.savefig('ROC.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "\n",
    "logit_y_test = pd.DataFrame()\n",
    "logit_y_test['prob'] = logit.predict(X_valid)\n",
    "\n",
    "logit_y_test['pred'] = 1\n",
    "logit_y_test.loc[logit_y_test.prob < 0.4, 'pred'] = 0\n",
    "\n",
    "Accuracy,FNR = conf_matrix(y_valid,logit_y_test['pred'])\n",
    "auc_train = auc_val(y_valid,logit_y_test['pred'])\n",
    "print('Validation Test Accuracy: {:.3f}'.format(Accuracy))\n",
    "print('Validation Test FNR: {:.3f}'.format(FNR))\n",
    "print('Validation Test AUC: {:.3f}'.format(auc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining the model for full training data\n",
    "\n",
    "logit = sm.Logit(train_data_Y_over, train_data_X_over).fit()\n",
    "\n",
    "logit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on test data\n",
    "\n",
    "pred = pd.DataFrame()\n",
    "pred['prob'] = logit.predict(test_data_X)\n",
    "\n",
    "pred['pred'] = 1\n",
    "pred.loc[pred['prob'] < 0.4, 'pred'] = 0 \n",
    "\n",
    "Accuracy_lr,FNR_lr = conf_matrix(test_data_Y,pred['pred'])\n",
    "\n",
    "lr_auc = auc_val(test_data_Y, pred['pred'])\n",
    "\n",
    "print('Test Accuracy: {:.3f}'.format(Accuracy_lr))\n",
    "print('Test FNR: {:.3f}'.format(FNR_lr))\n",
    "print('Test AUC: {:.3f}'.format(lr_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Support vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Support vector Classifier\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for best parameters\n",
    "\n",
    "best_fnr = 100\n",
    "\n",
    "c_val = [0.01, 0.1, 1, 10, 100]\n",
    "g_val = [0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "svc_train_auc = np.zeros((len(c_val), len(g_val)))\n",
    "svc_test_auc = np.zeros((len(c_val), len(g_val)))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for c in c_val:\n",
    "    j = 0\n",
    "    for gamma in g_val:\n",
    "        clf = SVC(kernel = 'rbf', C = c, gamma = gamma, random_state = 0)\n",
    "        clf.fit(X_train, y_train)\n",
    "        temp1_pred = clf.predict(X_train)\n",
    "        svc_train_auc[i,j] = auc_val(y_train, temp1_pred)\n",
    "        tr_acc = clf.score(X_train,y_train)\n",
    "        print('Training Accuracy : {:.3f}'.format(tr_acc))\n",
    "        temp2_pred = clf.predict(X_valid)\n",
    "        svc_test_auc[i,j] = auc_val(y_valid, temp2_pred)\n",
    "        Acc,fnr = conf_matrix(y_valid,temp2_pred)\n",
    "        print('---')\n",
    "        if (((Acc > 80) & (tr_acc < 1)) & (fnr < best_fnr)):\n",
    "            best_fnr = fnr\n",
    "            best_params = {'C' : c, 'gamma' : gamma}\n",
    "        j = j+1\n",
    "    i = i+1\n",
    "\n",
    "print('Best FNR : {:.2f}'.format(best_fnr))\n",
    "print('Best_parameters : {}'.format(best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC grids\n",
    "\n",
    "svc_train_auc = pd.DataFrame(svc_train_auc, index = c_val, columns = g_val)\n",
    "svc_test_auc = pd.DataFrame(svc_test_auc, index = c_val, columns = g_val)\n",
    "print('Training AUC')\n",
    "print(svc_train_auc)\n",
    "print('Test AUC')\n",
    "print(svc_test_auc)\n",
    "svc_test_auc.to_csv('SVC_AUC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting over full training data wit best parameters based on test_auc\n",
    "\n",
    "svc = SVC(kernel = 'rbf', C = 10, gamma = 0.001, random_state = 0)\n",
    "\n",
    "svc.fit(train_data_X_over,train_data_Y_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Score\n",
    "\n",
    "svc.score(train_data_X_over,train_data_Y_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on test data\n",
    "\n",
    "pred_svc = svc.predict(test_data_X)\n",
    "\n",
    "Accuracy_svc,FNR_svc = conf_matrix(test_data_Y,pred_svc)\n",
    "\n",
    "svc_auc = auc_val(test_data_Y,pred_svc)\n",
    "\n",
    "print('Test Accuracy: {:.3f}'.format(Accuracy_svc))\n",
    "print('Test FNR: {:.3f}'.format(FNR_svc))\n",
    "print('Test AUC: {:.3f}'.format(svc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosted Classifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for finding best parameters\n",
    "\n",
    "best_fnr = 100\n",
    "l_rate = [0.001, 0.01, 0.05, 0.1, 0.25, 0.5, 1]\n",
    "m_depth = [2, 4, 6, 8, 10, 12, 15]\n",
    "\n",
    "gbc_train_auc = np.zeros((len(l_rate),len(m_depth)))\n",
    "gbc_test_auc = np.zeros((len(l_rate),len(m_depth)))\n",
    "\n",
    "i=0\n",
    "\n",
    "for learn_rate in l_rate:\n",
    "    j = 0\n",
    "    for max_d in m_depth:\n",
    "        clf = GradientBoostingClassifier(max_depth = max_d, learning_rate = learn_rate, random_state = 0, max_features = 'sqrt')\n",
    "        clf.fit(X_train, y_train)\n",
    "        temp1_pred = clf.predict(X_train)\n",
    "        gbc_train_auc[i,j] = auc_val(y_train, temp1_pred)\n",
    "        tr_acc = clf.score(X_train,y_train)\n",
    "        print('Training Accuracy : {:.3f}'.format(clf.score(X_train,y_train)))\n",
    "        temp2_pred = clf.predict(X_valid)\n",
    "        gbc_test_auc[i,j] = auc_val(y_valid, temp2_pred)\n",
    "        Acc,fnr = conf_matrix(y_valid, temp2_pred)\n",
    "        print('---')\n",
    "        if (((Acc > 80) & (tr_acc < 1)) & (fnr < best_fnr)):\n",
    "            best_fnr = fnr\n",
    "            best_params = {'max_depth' : max_d, 'learning_rate' : learn_rate}\n",
    "        j = j+1\n",
    "    i = i+1\n",
    "\n",
    "print('Best FNR : {:.2f}'.format(best_fnr))\n",
    "print('Best_FNR_parameters : {}'.format(best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC grids\n",
    "\n",
    "gbc_train_auc = pd.DataFrame(gbc_train_auc, index = l_rate, columns = m_depth)\n",
    "gbc_test_auc = pd.DataFrame(gbc_test_auc, index = l_rate, columns = m_depth)\n",
    "print('Training AUC')\n",
    "print(gbc_train_auc)\n",
    "print('Test AUC')\n",
    "print(gbc_test_auc)\n",
    "gbc_test_auc.to_csv('GBC_AUC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting over complete training data with best parameters based on test_auc\n",
    "\n",
    "gbc = GradientBoostingClassifier(max_depth = 6, learning_rate = 0.01, random_state = 0, max_features = 'sqrt')\n",
    "\n",
    "gbc.fit(train_data_X_over,train_data_Y_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Score\n",
    "\n",
    "gbc.score(train_data_X_over,train_data_Y_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on test data\n",
    "\n",
    "pred_gbc = gbc.predict(test_data_X)\n",
    "\n",
    "Accuracy_gbc,FNR_gbc = conf_matrix(test_data_Y,pred_gbc)\n",
    "\n",
    "gbc_auc = auc_val(test_data_Y,pred_gbc)\n",
    "\n",
    "print('Test Accuracy: {:.3f}'.format(Accuracy_gbc))\n",
    "print('Test FNR: {:.3f}'.format(FNR_gbc))\n",
    "print('Test AUC: {:.3f}'.format(gbc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "\n",
    "result['Model'] = ['Decision Tree', 'Random Forest', 'Logistic Regression', 'Support vector Classifier', 'Gradient Boosted Classifier']\n",
    "result['Test Accuracy'] = [Accuracy_dt, Accuracy_rf, Accuracy_lr, Accuracy_svc, Accuracy_gbc]\n",
    "result['False negative rate'] = [FNR_dt, FNR_rf, FNR_lr, FNR_svc, FNR_gbc]\n",
    "result['Test AUC'] = [dt_auc, rf_auc, lr_auc, svc_auc, gbc_auc]\n",
    "result\n",
    "result.to_csv('Result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output using Selected Model i.e. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pred_rf).to_csv('Test_data_Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
